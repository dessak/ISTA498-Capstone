{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "__author__ = \"Nate Cutler\"\n",
    "__credits__ = [\"Data sourced from Bundesamt fur Strahlenschutz\"]\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Nate Cutler\"\n",
    "__email__ = \"ncutler211@gmail.com\"\n",
    "__status__ = \"Prototype\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_into_dataframe(directory):\n",
    "    \"\"\"\n",
    "    Reads data from .dat files in a specified directory into a Pandas DataFrame and combines them.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing the .dat files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame with data from all .dat files in the directory.\n",
    "\n",
    "    Example:\n",
    "        directory = \"F:/UofA/ISTA_498_Capstone/Data ETL/Germany/Extract/data/Uncompressed Original\"\n",
    "        combined_data = read_data_into_dataframe(directory)\n",
    "    \"\"\"\n",
    "    datafiles = []\n",
    "\n",
    "    # Iterate over each .dat file in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dat\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Read the .dat file into a DataFrame, specifying delimiter and encoding\n",
    "            df = pd.read_csv(file_path, delimiter='|', encoding='latin1')\n",
    "\n",
    "            # Rename the DataFrame columns to standard names\n",
    "            new_column_names = [\"Postal code\", \"city\", \"start_date\", \"reading\"]\n",
    "            df.rename(columns=dict(zip(df.columns, new_column_names)), inplace=True)\n",
    "\n",
    "            # Append the DataFrame to the list of datafiles\n",
    "            datafiles.append(df)\n",
    "\n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    combined_data = pd.concat(datafiles, ignore_index=True)\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"F:\\UofA\\ISTA_498_Capstone\\Data ETL\\Germany\\Extract\\data\\Uncompressed Original\"\n",
    "combined_data = read_data_into_dataframe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations(combined_data):\n",
    "    \"\"\"\n",
    "    Perform data transformations on a Pandas DataFrame.\n",
    "\n",
    "    This function applies several transformations to the input DataFrame to prepare it for further analysis or storage.\n",
    "\n",
    "    Parameters:\n",
    "        combined_data (pd.DataFrame): The input DataFrame to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    # Create unit column with microsievert per hour\n",
    "    combined_data[\"unit\"] = 'Î¼Sv/h'\n",
    "    # Drop Postal Code column\n",
    "    combined_data = combined_data.drop(\"Postal code\", axis=1)\n",
    "    # Convert Date column to datetime\n",
    "    combined_data['start_date'] = pd.to_datetime(combined_data['start_date'])\n",
    "    # Add End_Date column. All data is daily data, so no need for sampling over time.\n",
    "    combined_data['end_date'] = combined_data['start_date']\n",
    "    # Add CID column\n",
    "    combined_data['cid'] = '04'\n",
    "    # Add State column (it's initially empty)\n",
    "    combined_data['state'] = \"\"\n",
    "    # Add Latitude column (initially filled with NaN)\n",
    "    combined_data['lat'] = np.nan\n",
    "    # Add Longitude column (initially filled with NaN)\n",
    "    combined_data['long'] = np.nan\n",
    "    # Add Comment column (it's initially empty)\n",
    "    combined_data['comment'] = \"\"\n",
    "    # Reorganize columns to a specific order\n",
    "    combined_data = combined_data[[\"start_date\", \"end_date\", \"reading\", \"unit\", \"city\", \"state\", \"cid\", \"lat\", \"long\", \"comment\"]]\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transformations(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for NaNs\n",
    "nan_check = transformed_data[\"reading\"].isna()\n",
    "nan_count = nan_check.sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NaNs from dataset. Due to German Copyright laws I have chosen not to average values for missing days\n",
    "transformed_data.dropna(subset=['reading'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to a monthly average\n",
    "resampled_data = transformed_data.resample('M', on='start_date').mean()\n",
    "\n",
    "# Create the area plot\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.fill_between(resampled_data.index, resampled_data['reading'], alpha=0.5)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Average Reading')\n",
    "plt.title('Daily Average Reading Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to a monthly average\n",
    "resampled_data = transformed_data.resample('M', on='start_date').mean()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(resampled_data.index, resampled_data['reading'], marker='o', linestyle='-')  # Modify the linestyle here\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Average Reading')\n",
    "plt.title('Daily Average Reading Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset ~ size requirment\n",
    "print(f\"Total: {transformed_data.memory_usage(deep=True).sum()/1e+9} Gigabyte(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Connection to DB prints version if successful\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"radiance-db-instance.cdolgqfaeaoj.us-east-2.rds.amazonaws.com\",\n",
    "        database=\"Radiance_db\",\n",
    "        user=\"radianceUN\",\n",
    "        password=\"radianceP\",\n",
    "        port='5432'\n",
    "    )\n",
    "\n",
    "    # Create a cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Execute a test query\n",
    "    cur.execute(\"SELECT version();\")\n",
    "\n",
    "    # Fetch and print the result\n",
    "    db_version = cur.fetchone()\n",
    "    print(\"PostgreSQL database version:\")\n",
    "    print(db_version)\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error connecting to the database:\", e)\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for traversing host\n",
    "host=\"radiance-db-instance.cdolgqfaeaoj.us-east-2.rds.amazonaws.com\"\n",
    "database=\"Radiance_db\"\n",
    "user=\"radianceUN\"\n",
    "password=\"radianceP\"\n",
    "port='5432'\n",
    "\n",
    "def get_conn_cur(): # define function name and arguments (there aren't any)\n",
    "  # Make a connection\n",
    "  conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    database=database,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port='5432')\n",
    "\n",
    "  cur = conn.cursor()   # Make a cursor after\n",
    "\n",
    "  return(conn, cur)   # Return both the connection and the cursor\n",
    "\n",
    "def get_table_names():\n",
    "  conn, cur = get_conn_cur() # get connection and cursor\n",
    "\n",
    "  # query to get table names\n",
    "  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n",
    "       WHERE table_schema = 'public' \"\"\"\n",
    "\n",
    "  cur.execute(table_name_query) # execute\n",
    "  my_data = cur.fetchall() # fetch results\n",
    "\n",
    "  cur.close() #close cursor\n",
    "  conn.close() # close connection\n",
    "\n",
    "  return(my_data) # return your fetched results\n",
    "\n",
    "def get_column_names(table_name): # arguement of table_name\n",
    "  conn, cur = get_conn_cur() # get connection and cursor\n",
    "\n",
    "  # Now select column names while inserting the table name into the WERE\n",
    "  column_name_query =  \"\"\"SELECT column_name FROM information_schema.columns\n",
    "       WHERE table_name = '%s' \"\"\" %table_name\n",
    "\n",
    "  cur.execute(column_name_query) # exectue\n",
    "  my_data = cur.fetchall() # store\n",
    "\n",
    "  cur.close() # close\n",
    "  conn.close() # close\n",
    "\n",
    "  return(my_data) # return\n",
    "\n",
    "def run_query(query_string):\n",
    "\n",
    "  conn, cur = get_conn_cur() # get connection and cursor\n",
    "\n",
    "  cur.execute(query_string) # executing string as before\n",
    "\n",
    "  my_data = cur.fetchall() # fetch query data as before\n",
    "\n",
    "  # here we're extracting the 0th element for each item in cur.description\n",
    "  colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "  cur.close() # close\n",
    "  conn.close() # close\n",
    "\n",
    "  return(colnames, my_data) # return column names AND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(host, database, user, password, table_name, table_definition):\n",
    "    \"\"\"\n",
    "    Create a new table in a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        host (str): The host or IP address of the database server.\n",
    "        database (str): The name of the database.\n",
    "        user (str): The database user.\n",
    "        password (str): The user's password.\n",
    "        table_name (str): The name of the new table to be created.\n",
    "        table_definition (str): The table's column definitions and constraints.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = psycopg2.connect(\n",
    "            host=host,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "        # Create a cursor\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Define the SQL command to create the table\n",
    "        create_table_query = f\"CREATE TABLE {table_name} {table_def}\"\n",
    "\n",
    "        # Execute the SQL command\n",
    "        cur.execute(create_table_query)\n",
    "\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the cursor and the connection\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Germany table\n",
    "table_name = \"germany\"\n",
    "table_def = \"(start_date TIMESTAMP,end_date TIMESTAMP, reading FLOAT, unit VARCHAR(12), city VARCHAR(255), state VARCHAR(30), cid VARCHAR(3), lat INT, long INT, comment VARCHAR(255))\"\n",
    "create_table(host, database, user, password, table_name, table_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(table_name):\n",
    "    # Define your database URI\n",
    "    database_uri = \"postgresql://radianceUN:radianceP@radiance-db-instance.cdolgqfaeaoj.us-east-2.rds.amazonaws.com:5432/Radiance_db\"\n",
    "    engine = create_engine(database_uri)\n",
    "\n",
    "    # Connect to the database and drop the table\n",
    "    connection = engine.connect()\n",
    "    connection.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 'germany'\n",
    "drop_table(g)\n",
    "get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_head(table_name):\n",
    "  conn, cur = get_conn_cur()\n",
    "  sales_df = pd.read_sql(\"\"\" SELECT * FROM %s\n",
    "          LIMIT 5;\"\"\" %table_name, conn)\n",
    "  return (sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas DF to SQL table & Upload\n",
    "database_uri = \"postgresql://radianceUN:radianceP@radiance-db-instance.cdolgqfaeaoj.us-east-2.rds.amazonaws.com:5432/Radiance_db\"\n",
    "engine = create_engine(database_uri)\n",
    "tn = \"germany\"\n",
    "\n",
    "transformed_data_subset = transformed_data.head(10)  # Limit to the first 10 rows\n",
    "transformed_data_subset.to_sql(tn, engine, if_exists='replace', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
